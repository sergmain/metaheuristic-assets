functions:
  - code: simple-rnn:1.3
    type: simple-rnn
    env: python-3
    sourcing: processor
    metas:
      - mh.task-params-version: 1
      - mh.function-params-as-file: true
      - mh.function-params-file-ext: .py
    params: |+
      import matplotlib.pyplot as plt
      import math
      from sklearn.preprocessing import MinMaxScaler
      from sklearn.metrics import mean_squared_error

      import gc
      import os
      import sys
      from datetime import datetime

      import keras
      import numpy
      import pandas
      import yaml
      from sklearn.preprocessing import MinMaxScaler

      from tensorflow.keras.layers import *
      from tensorflow.keras.metrics import top_k_categorical_accuracy
      from tensorflow.keras.models import Sequential
      from tensorflow.keras.optimizers import *
      from tensorflow.keras import backend as K
      import tensorflow as tf


      class Logger(object):
          def __init__(self, artifact_path):
              self.terminal = sys.stdout
              self.log = open(os.path.join(artifact_path, "logfile-fit.log"), "w", encoding="utf-8")

          def write(self, message):
              self.terminal.write(message)
              self.log.write(message)
              self.log.flush()

          def flush(self):
              # this flush method is needed for python 3 compatibility.
              # this handles the flush command by doing nothing.
              # you might want to specify some extra behavior here.
              pass


      def get_variable_by_id(variables, code):
          for v in variables:
              if v['name']==code:
                  return v

          raise Exception('A variable ' + code + " wasn't found")


      def get_variable_by_type(variables, type):
          for v in variables:
              if v['type']==type:
                  return v

          raise Exception('A variable with type ' + type + " wasn't found" )



      def _load_data(data, n_prev=100):
          docX, docY = [], []
          for i in range(len(data) - n_prev):
              docX.append(data[i:i + n_prev])
              docY.append(data[i + n_prev])
          alsX = numpy.array(docX)
          alsY = numpy.array(docY)

          return alsX, alsY


      def train_test_split(df, time_steps, train_size, test_size):
          X_train, y_train = _load_data(df[0:train_size + time_steps], n_prev=time_steps)
          X_test, y_test = _load_data(df[train_size:train_size + test_size + time_steps], n_prev=time_steps)

          return (X_train, y_train), (X_test, y_test)


      def get_features(params):
          feature_item = get_variable_by_id(params['inputs'], 'var-dataset')

          with open(os.path.join('variable', str(feature_item['id'])), 'r', encoding="utf-8") as stream:
              variables = (yaml.load(stream, Loader=yaml.FullLoader))['array']
              features = []
              for f in variables:
                  features.append(os.path.join(str(f['dataType']), str(f['id'])))

              return features


      ######

      print('Start time: ', str(datetime.now()))
      print('Keras version: ', keras.__version__)
      print('TF version: ', tf.__version__)
      print('Args: ', sys.argv)


      cwd = os.getcwd()

      artifact_path = os.path.join(cwd, 'artifacts')
      sys.stdout = Logger(artifact_path)
      sys.stderr = sys.stdout

      # path to params.yaml will be absolute
      yaml_file = sys.argv[len(sys.argv)-1]

      with open(yaml_file, 'r', encoding="utf-8") as stream:
          params = (yaml.load(stream, Loader=yaml.FullLoader))['task']

      hyperParams = params['inline']['mh.hyper-params']
      print(hyperParams)

      epochs = int(hyperParams['epoch'])
      time_steps = int(hyperParams['time_steps'])
      batch_size = int(hyperParams['batch_size'])

      # Try with replacing by LSTM, GRU, or SimpleRNN.
      RNN = globals()[hyperParams['RNN']]
      metrics_functions = hyperParams.get('metrics_functions')

      test_size_percent = 0.20

      # fix random seed for reproducibility
      numpy.random.seed(42)

      var_dataset = get_variable_by_id(params['inputs'], 'var-dataset')
      dataset_filename = os.path.join(str(var_dataset['dataType']), str(var_dataset['id']))

      features = get_features(params)

      for feature_file_name in features:
          dataframe = pandas.read_csv(feature_file_name, engine='python', header=None)
          break


      print("dataset length: " + str(len(dataframe.values)))

      dataset = dataframe.values
      dataset = dataset.astype('float32')
      # normalize the dataset
      scaler = MinMaxScaler(feature_range=(0, 1))
      dataset = scaler.fit_transform(dataset)

      effective_count = int((len(dataset) - time_steps) / batch_size) * batch_size
      train_size = int(effective_count * (1 - test_size_percent))
      train_size = int(train_size / batch_size) * batch_size
      test_size = effective_count - train_size
      print("len(dataset): {}, batch_size: {}, effective_count: {}, train_size: {}, test_size: {}".format(len(dataset), batch_size, effective_count, train_size, test_size))

      (trainX, trainY), (testX, testY) = train_test_split(dataset, time_steps, train_size, test_size)

      col_number = dataframe.shape[1]

      #########

      all_metrics = []
      if metrics_functions is not None:
          for n in metrics_functions.split(','):
              nn = n.strip()
              if len(nn)==0:
                  continue
              if nn.startswith('#'):
                  all_metrics.append(eval(nn[1:]))
              else:
                  all_metrics.append(nn)

      if len(all_metrics)==0:
          all_metrics.append('accuracy')


      activation = hyperParams['activation']
      # https://github.com/fchollet/keras/blob/master/keras/optimizers.py
      optimizer = hyperParams['optimizer']
      loss='mean_squared_error'
      # loss = 'categorical_crossentropy'
      # loss = 'binary_crossentropy'


      # create and fit the LSTM network
      model = Sequential()
      model.add(RNN(100, batch_input_shape=(batch_size, time_steps, col_number), stateful=True))
      model.add(Dense(col_number))
      model.add(Activation(activation))
      model.compile(loss=loss, optimizer=optimizer, metrics=all_metrics)

      print("with Activation('{}'), optimizer='{}', loss='{}'".format(activation, optimizer, loss))
      model.summary()

      for i in range(epochs):
          model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)
          model.reset_states()
          print("Actual epoch {}/{}".format(i + 1, epochs))


      ### Metrics
      # make predictions
      trainPredict = model.predict(trainX, batch_size)
      testPredict = model.predict(testX, batch_size)
      # invert predictions
      trainPredict = scaler.inverse_transform(trainPredict)
      trainY = scaler.inverse_transform(trainY)
      testPredict = scaler.inverse_transform(testPredict)
      testY = scaler.inverse_transform(testY)

      # calculate root mean squared error
      trainScore = math.sqrt(mean_squared_error(trainY[:, 0], trainPredict[:, 0]))
      print('Train Score: %.2f RMSE' % trainScore)
      testScore = math.sqrt(mean_squared_error(testY[:, 0], testPredict[:, 0]))
      print('Test Score: %.2f RMSE' % testScore)

      metrics = {}
      metricValues = {}
      metrics['values'] = metricValues

      metricValues['Train Score'] = trainScore
      metricValues['Test Score'] = testScore

      var_metrics = get_variable_by_id(params['outputs'], 'var-metrics')
      metrics_yaml_file = os.path.join(artifact_path, str(var_metrics['id']))

      with open(metrics_yaml_file, 'w', encoding="utf-8") as outfile:
          yaml.dump(metrics, outfile, default_flow_style=False)

      ### Prediction
      prediction_data = {}
      predicted_overfitting = []
      expected_overfitting = []
      prediction_data['predicted'] = predicted_overfitting
      prediction_data['expected'] = expected_overfitting

      prediction_data_yaml_file = os.path.join(artifact_path, str(get_variable_by_id(params['outputs'], 'var-predicted')['id']))

      with open(prediction_data_yaml_file, 'w', encoding="utf-8") as outfile:
          yaml.dump(prediction_data, outfile, default_flow_style=False)

      ### Fitting
      result = {}
      result['fitting'] = 'NORMAL'

      var_fitting = get_variable_by_id(params['outputs'], 'var-fitting')
      fitting_result_file = os.path.join(artifact_path, str(var_fitting['id']))

      with open(fitting_result_file, 'w', encoding="utf-8") as outfile:
          yaml.dump(result, outfile, default_flow_style=False)


      print('Done.')
      print(str(datetime.now()))
      K.clear_session()
      gc.collect()

      sys.exit(0)

